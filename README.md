# ğŸ§  BLIP + LLaMA 3 Image Captioning Pipeline

This project demonstrates a powerful **multimodal AI system** that combines **BLIP** (Bootstrapped Language Image Pretraining) for image captioning and **LLaMA 3** (Meta's state-of-the-art large language model) for instruction-following. Built using Hugging Face Transformers and executed on Google Colab with free GPU support.

---

## ğŸ“Œ What This Project Does

- ğŸ“¸ Takes any input image  
- âœï¸ Uses **BLIP** to generate a detailed caption  
- ğŸ’¬ Feeds the caption into **LLaMA 3**, which can:  
  - Answer questions about the image  
  - Expand on image context  
  - Follow user instructions  

---

## ğŸ› ï¸ Technologies Used

- `transformers` by Hugging Face ğŸ¤—  
- **BLIP** for visual understanding and captioning  
- **Meta LLaMA 3 8B-Instruct** for text generation  
- `torch`, `accelerate`, `bitsandbytes`  
- Google Colab for execution  
- Hugging Face Hub for model access


## ğŸŒ Real-World Applications
- ğŸ‘¶ Children's Education â€“ Describe images for interactive learning
- ğŸ§‘â€âš•ï¸ Healthcare â€“ Auto-caption medical scans
- ğŸ›’ E-Commerce â€“ Generate product descriptions from photos
- â™¿ Accessibility â€“ Help visually impaired users understand images
- ğŸ“° Journalism & Content Creation â€“ Auto-generate context and headlines

## ğŸ™Œ Credits
- ğŸ¤— Hugging Face for the open-source models and tools
- ğŸ§  Meta AI for LLaMA 3
- ğŸ”¬ Salesforce Research for BLIP

## ğŸ’¬ Contact
- Project by Ajmal
- ğŸ“« Email: 24f2004489@ds.study.iitm.ac.in
- ğŸ§ª Built for educational and research purposes only.
