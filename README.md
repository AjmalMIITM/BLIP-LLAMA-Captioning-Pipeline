# 🧠 BLIP + LLaMA 3 Image Captioning Pipeline

This project demonstrates a powerful **multimodal AI system** that combines **BLIP** (Bootstrapped Language Image Pretraining) for image captioning and **LLaMA 3** (Meta's state-of-the-art large language model) for instruction-following. Built using Hugging Face Transformers and executed on Google Colab with free GPU support.

---

## 📌 What This Project Does

- 📸 Takes any input image  
- ✍️ Uses **BLIP** to generate a detailed caption  
- 💬 Feeds the caption into **LLaMA 3**, which can:  
  - Answer questions about the image  
  - Expand on image context  
  - Follow user instructions  

---

## 🛠️ Technologies Used

- `transformers` by Hugging Face 🤗  
- **BLIP** for visual understanding and captioning  
- **Meta LLaMA 3 8B-Instruct** for text generation  
- `torch`, `accelerate`, `bitsandbytes`  
- Google Colab for execution  
- Hugging Face Hub for model access


## 🌍 Real-World Applications
- 👶 Children's Education – Describe images for interactive learning
- 🧑‍⚕️ Healthcare – Auto-caption medical scans
- 🛒 E-Commerce – Generate product descriptions from photos
- ♿ Accessibility – Help visually impaired users understand images
- 📰 Journalism & Content Creation – Auto-generate context and headlines

## 🙌 Credits
- 🤗 Hugging Face for the open-source models and tools
- 🧠 Meta AI for LLaMA 3
- 🔬 Salesforce Research for BLIP

## 💬 Contact
- Project by Ajmal
- 📫 Email: 24f2004489@ds.study.iitm.ac.in
- 🧪 Built for educational and research purposes only.
